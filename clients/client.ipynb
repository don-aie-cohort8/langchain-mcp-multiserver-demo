{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "535209f8",
   "metadata": {},
   "source": [
    "# LangChain MCP Adapters - Learning Notebook\n",
    "\n",
    "This notebook demonstrates how to integrate LangChain agents with MCP (Model Context Protocol) servers.\n",
    "\n",
    "## Learning Path\n",
    "1. Single MCP server connection (stdio transport)\n",
    "2. Multiple MCP servers (stdio + streamable-http transports)\n",
    "3. Streamable HTTP transport\n",
    "4. Passing runtime headers\n",
    "5. Using with LangGraph StateGraph\n",
    "6. Converting LangChain tools to MCP format\n",
    "\n",
    "## Prerequisites\n",
    "- Start the weather server: `python servers/weather_server.py`\n",
    "- Start the langchain tools server: `python servers/langchain_tools_server.py --port 8001`\n",
    "- Refer to the [Quickstart in the README.md](../README.md) for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9107af23",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec188363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Optional\n",
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n",
    "from display_utils import display_agent_response, get_final_answer, print_tools_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9467636",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b57c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create server parameters for stdio connection\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "# from langgraph.prebuilt import create_react_agent\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    # Make sure to update to the full absolute path to your math_server.py file\n",
    "    args=[\"/home/donbr/don-aie-cohort8/aie8-s13-langchain-mcp/servers/math_server.py\"],\n",
    ")\n",
    "\n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        # Initialize the connection\n",
    "        await session.initialize()\n",
    "\n",
    "        # Get tools\n",
    "        tools = await load_mcp_tools(session)\n",
    "\n",
    "        # Create and run the agent\n",
    "        # prior approach using LangGraph ReAct agent:\n",
    "        # agent = create_agent(\"openai:gpt-4.1\", tools)\n",
    "        agent = create_agent(\"openai:gpt-4.1\", tools)\n",
    "        agent_response = await agent.ainvoke({\"messages\": \"what's (3 + 5) x 12?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac60ab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print tools summary\n",
    "print_tools_summary(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f57d4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using stdio transport: the client spawns the MCP server as a subprocess.\n",
    "# The server runs only for the lifetime of the session and terminates automatically\n",
    "# when the async context ends.\n",
    "\n",
    "# we expect the following asynchronousrequest to fail:\n",
    "\n",
    "agent_response = await agent.ainvoke({\"messages\": [HumanMessage(\"what's (3 + 5) x 12?\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qpw6nt2x1gq",
   "metadata": {},
   "source": [
    "### Understanding ClosedResourceError\n",
    "\n",
    "**What you're seeing:** `ClosedResourceError()` in the tool responses above.\n",
    "\n",
    "**Why it happens:** The MCP session is closed after the initial tool discovery phase because we used an `async with` context manager. When the agent tries to call the tools, the session is no longer available.\n",
    "\n",
    "**The fix:** Use `MultiServerMCPClient` (shown in later examples) which manages session lifecycle properly, keeping connections open for tool invocations.\n",
    "\n",
    "**Key lesson:** When using stdio transport with context managers, the session must remain open during agent execution. The `MultiServerMCPClient` pattern handles this correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4408d0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full trace (my version of pretty print)\n",
    "display_agent_response(agent_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107dfa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# but if you want to see the ugly full trace and messages:\n",
    "\n",
    "# print(agent_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8e8469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just the answer\n",
    "display_agent_response(agent_response, show_full_trace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341daf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get answer as variable\n",
    "answer = display_agent_response(agent_response, return_final_answer=True)\n",
    "print(f\"The answer is: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae883d2",
   "metadata": {},
   "source": [
    "## Multiple MCP Servers\n",
    "\n",
    "The library also allows you to connect to multiple MCP servers and load tools from them:\n",
    "\n",
    "- math_server.py\n",
    "- weather_server.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27000dfb",
   "metadata": {},
   "source": [
    "### IMPORTANT: RUN WEATHER SERVER IN A SEPARATE TERMINAL\n",
    "\n",
    "```bash\n",
    "python servers/weather_server.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02db203",
   "metadata": {},
   "source": [
    "### Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce182a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "# from langgraph.prebuilt import create_react_agent\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"math\": {\n",
    "            \"command\": \"python\",\n",
    "            # Make sure to update to the full absolute path to your math_server.py file\n",
    "            \"args\": [\"/home/donbr/don-aie-cohort8/aie8-s13-langchain-mcp/servers/math_server.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "        \"weather\": {\n",
    "            # Make sure you start your weather server on port 8000\n",
    "            \"url\": \"http://localhost:8000/mcp\",\n",
    "            \"transport\": \"streamable_http\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "tools = await client.get_tools()\n",
    "agent = create_agent(\"openai:gpt-4.1\", tools)\n",
    "math_response = await agent.ainvoke({\"messages\": \"what's (3 + 5) x 12?\"})\n",
    "weather_response = await agent.ainvoke({\"messages\": \"what is the weather in nyc?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c71640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display tools\n",
    "print_tools_summary(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2b4f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full trace\n",
    "display_agent_response(math_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b9b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just the answer\n",
    "display_agent_response(weather_response, show_full_trace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e389cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get answer as variable\n",
    "answer = display_agent_response(math_response, return_final_answer=True)\n",
    "print(f\"The answer is: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5722a47b",
   "metadata": {},
   "source": [
    "> [!note]\n",
    "> Example above will start a new MCP `ClientSession` for each tool invocation. If you would like to explicitly start a session for a given server, you can do:\n",
    ">\n",
    "> ```python\n",
    "> from langchain_mcp_adapters.tools import load_mcp_tools\n",
    ">\n",
    "> client = MultiServerMCPClient({...})\n",
    "> async with client.session(\"math\") as session:\n",
    ">     tools = await load_mcp_tools(session)\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbd7e64",
   "metadata": {},
   "source": [
    "## Streamable HTTP\n",
    "\n",
    "MCP now supports [streamable HTTP](https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#streamable-http) transport.\n",
    "\n",
    "To start an [example](https://github.com/langchain-ai/langchain-mcp-adapters/tree/main/examples/servers/streamable-http-stateless) streamable HTTP server, run the following:\n",
    "\n",
    "```bash\n",
    "cd /home/donbr/don-aie-cohort8/aie8-s13-langchain-mcp/examples/servers/streamable-http-stateless/\n",
    "uv run mcp-simple-streamablehttp-stateless --port 3000\n",
    "```\n",
    "\n",
    "Alternatively, you can use FastMCP directly (as in the examples above).\n",
    "\n",
    "To use it with Python MCP SDK `streamablehttp_client`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e13376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use server from examples/servers/streamable-http-stateless/\n",
    "\n",
    "from mcp import ClientSession\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "\n",
    "# from langgraph.prebuilt import create_react_agent\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "\n",
    "async with streamablehttp_client(\"http://localhost:3000/mcp\") as (read, write, _):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        # Initialize the connection\n",
    "        await session.initialize()\n",
    "\n",
    "        # Get tools\n",
    "        tools = await load_mcp_tools(session)\n",
    "        agent = create_agent(\"openai:gpt-4.1\", tools)\n",
    "        # HINT:\n",
    "        # agent = create_agent(\"openai:gpt-4.1\", tools)\n",
    "        math_response = await agent.ainvoke({\"messages\": \"what's (3 + 5) x 12?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34eb1ae",
   "metadata": {},
   "source": [
    "Use it with `MultiServerMCPClient`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cbb9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use server from examples/servers/streamable-http-stateless/\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "# from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"math\": {\n",
    "            \"transport\": \"streamable_http\",\n",
    "            \"url\": \"http://localhost:3000/mcp\"\n",
    "        },\n",
    "    }\n",
    ")\n",
    "tools = await client.get_tools()\n",
    "agent = create_agent(\"openai:gpt-4.1\", tools)\n",
    "# HINT:\n",
    "# agent = create_agent(\"openai:gpt-4.1\", tools)\n",
    "math_response = await agent.ainvoke({\"messages\": \"what's (3 + 5) x 12?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac62841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the tools summary\n",
    "print_tools_summary(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427d1927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the agent response\n",
    "display_agent_response(math_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ff45ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the final answer\n",
    "final_answer = get_final_answer(math_response)\n",
    "print(f\"The final answer is: {final_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9120b6d",
   "metadata": {},
   "source": [
    "## Passing runtime headers\n",
    "\n",
    "When connecting to MCP servers, you can include custom headers (e.g., for authentication or tracing) using the `headers` field in the connection configuration. This is supported for the following transports:\n",
    "\n",
    "- `sse`\n",
    "- `streamable_http`\n",
    "\n",
    "### Example: passing headers with `MultiServerMCPClient`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bf4b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"weather\": {\n",
    "            \"transport\": \"streamable_http\",\n",
    "            \"url\": \"http://localhost:8000/mcp\",\n",
    "            \"headers\": {\n",
    "                \"Authorization\": \"Bearer YOUR_TOKEN\",\n",
    "                \"X-Custom-Header\": \"custom-value\"\n",
    "            },\n",
    "        }\n",
    "    }\n",
    ")\n",
    "tools = await client.get_tools()\n",
    "agent = create_agent(\"openai:gpt-4.1\", tools)\n",
    "# HINT:\n",
    "# agent = create_agent(\"openai:gpt-4.1\", tools)\n",
    "response = await agent.ainvoke({\"messages\": \"what is the weather in nyc?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa342c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the tools summary\n",
    "print_tools_summary(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a362981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the agent response\n",
    "display_agent_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734a555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the final answer\n",
    "final_answer = get_final_answer(response)\n",
    "print(f\"The final answer is: {final_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e95242",
   "metadata": {},
   "source": [
    "> Only `sse` and `streamable_http` transports support runtime headers. These headers are passed with every HTTP request to the MCP server.\n",
    "\n",
    "## Using with LangGraph StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e01c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"openai:gpt-4.1\")\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"math\": {\n",
    "            \"command\": \"python\",\n",
    "            # Make sure to update to the full absolute path to your math_server.py file\n",
    "            \"args\": [\"/home/donbr/don-aie-cohort8/aie8-s13-langchain-mcp/servers/math_server.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "        \"weather\": {\n",
    "            # make sure you start your weather server on port 8000\n",
    "            \"url\": \"http://localhost:8000/mcp\",\n",
    "            \"transport\": \"streamable_http\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "tools = await client.get_tools()\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.bind_tools(tools).invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(call_model)\n",
    "builder.add_node(ToolNode(tools))\n",
    "builder.add_edge(START, \"call_model\")\n",
    "builder.add_conditional_edges(\n",
    "    \"call_model\",\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"call_model\")\n",
    "graph = builder.compile()\n",
    "math_response = await graph.ainvoke({\"messages\": \"what's (3 + 5) x 12?\"})\n",
    "weather_response = await graph.ainvoke({\"messages\": \"what is the weather in nyc?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecd0f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print tools summary\n",
    "print_tools_summary(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e22f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the math agent response\n",
    "display_agent_response(math_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39afa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the weather agent response\n",
    "display_agent_response(weather_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab42c4d6",
   "metadata": {},
   "source": [
    "## Using with LangGraph API Server\n",
    "\n",
    "> [!TIP]\n",
    "> Check out [this guide](https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/) on getting started with LangGraph API server.\n",
    "\n",
    "If you want to run a LangGraph agent that uses MCP tools in a LangGraph API server, you can use the following setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9472949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph.py\n",
    "from contextlib import asynccontextmanager\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "async def make_graph():\n",
    "    client = MultiServerMCPClient(\n",
    "        {\n",
    "            \"math\": {\n",
    "                \"command\": \"python\",\n",
    "                # Make sure to update to the full absolute path to your math_server.py file\n",
    "                \"args\": [\"/home/donbr/don-aie-cohort8/aie8-s13-langchain-mcp/servers/math_server.py\"],\n",
    "                \"transport\": \"stdio\",\n",
    "            },\n",
    "            \"weather\": {\n",
    "                # make sure you start your weather server on port 8000\n",
    "                \"url\": \"http://localhost:8000/mcp\",\n",
    "                \"transport\": \"streamable_http\",\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    tools = await client.get_tools()\n",
    "    agent = create_agent(\"openai:gpt-4.1\", tools)\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c4b663",
   "metadata": {},
   "source": [
    "In your [`langgraph.json`](https://langchain-ai.github.io/langgraph/cloud/reference/cli/#configuration-file) make sure to specify `make_graph` as your graph entrypoint:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"dependencies\": [\".\"],\n",
    "  \"graphs\": {\n",
    "    \"agent\": \"./graph.py:make_graph\"\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182678d4",
   "metadata": {},
   "source": [
    "## Add LangChain tools to a FastMCP server\n",
    "\n",
    "Use `to_fastmcp` to convert LangChain tools to FastMCP, and then add them to the `FastMCP` server via the initializer:\n",
    "\n",
    "> [!NOTE]\n",
    "> `tools` argument is only available in FastMCP as of `mcp >= 1.9.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9dd83b",
   "metadata": {},
   "source": [
    "### RUN IN A SEPARATE TERMINAL\n",
    "\n",
    "```bash\n",
    "python servers/langchain_tools_server.py --port 8001\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddebfeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from display_utils import display_agent_response\n",
    "\n",
    "client = MultiServerMCPClient({\n",
    "    \"langchain_math\": {\n",
    "        \"url\": \"http://localhost:8001/mcp\",\n",
    "        \"transport\": \"streamable_http\",\n",
    "    }\n",
    "})\n",
    "\n",
    "tools = await client.get_tools()\n",
    "agent = create_agent(\"openai:gpt-4.1\", tools)\n",
    "response = await agent.ainvoke({\"messages\": \"what is 15 + 27?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d141cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print tools summary\n",
    "print_tools_summary(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b5a57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_agent_response(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
